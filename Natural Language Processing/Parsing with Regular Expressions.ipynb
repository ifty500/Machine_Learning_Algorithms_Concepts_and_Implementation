{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Discovering new code words in declassified CIA documents may seem like a mission for a foreign intelligence service, and detecting gender biases in the Harry Potter novels a task for a literature professor. Yet by utilizing natural language parsing with regular expressions, the power to perform such analyses is in your own hands!\n",
    "\n",
    "While you may not put much explicit thought into the structure of your sentences as you write, the syntax choices you make are critical in ensuring your writing has meaning. Analyzing such sentence structure as well as word choice can not only provide insights into the connotation of a piece text, but can also highlight the biases of its author or uncover additional insights that even a deep, rigorous reading of the text might not reveal.\n",
    "\n",
    "By using Python’s regular expression modulere and the Natural Language Toolkit, known as NLTK, you can find keywords of interest, discover where and how often they are used, and discern the parts-of-speech patterns in which they appear to understand the sometimes hidden meaning in a piece of writing. Let’s get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'np_chunk_counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-689f53bcb353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#from pos_tagged_oz import pos_tagged_oz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnp_chunk_counter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_chunk_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# define noun-phrase chunk grammar here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'np_chunk_counter'"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "#from pos_tagged_oz import pos_tagged_oz\n",
    "from np_chunk_counter import np_chunk_counter\n",
    "\n",
    "# define noun-phrase chunk grammar here\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold noun-phrase chunked sentences\n",
    "np_chunked_oz = list()\n",
    "\n",
    "# create a for-loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to np_chunked_oz here\n",
    "  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the most common np-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and Matching\n",
    "\n",
    "Before you dive into more complex syntax parsing, you’ll begin with basic regular expressions in Python using the re module as a regex refresher.\n",
    "\n",
    "The first method you will explore is .compile(). This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text. The regular expression object below will exactly match 4 upper or lower case characters.\n",
    "\n",
    "    regular_expression_object = re.compile(\"[A-Za-z]{4}\")\n",
    "    \n",
    "Regular expression objects have a .match() method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string. To see if your regular expression matches the string \"Toto\" you can do the following:\n",
    "\n",
    "    result = regular_expression_object.match(\"Toto\")\n",
    "    \n",
    "If .match() finds a match that starts at the beginning of the string, it will return a match object. The match object lets you know what piece of text the regular expression matched, and at what index the match begins and ends. If there is no match, .match() will return None.\n",
    "\n",
    "With the match object stored in result, you can access the matched text by calling result.group(0). If you use a regex containing capture groups, you can access these groups by calling .group() with the appropriately numbered capture group as an argument.\n",
    "\n",
    "Instead of compiling the regular expression first and then looking for a match in separate lines of code, you can simplify your match to one line:\n",
    "\n",
    "    result = re.match(\"[A-Za-z]{4}\",\"Toto\")\n",
    "    \n",
    "With this syntax, re‘s .match() method takes a regular expression pattern as the first argument and a string as the second argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "The re module has been imported for you at the top of the workspace. .compile() a regular expression object named regular_expression that will match any 7 character string of word characters.\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Use regular_expression‘s .match() method to check if the regex matches the string stored in character_1. Save the result to result_1 and print it.\n",
    "\n",
    "\n",
    "3.\n",
    "Access the match in result_1 using its .group() method with an argument of 0. Save the result to match_1 and print it.\n",
    "\n",
    "\n",
    "4.\n",
    "In one line, use re‘s .match() method to compile a regular expression that will match any string of characters of length 7 and check if the regex matches the string stored in character_2. Save the result to result_2 and print it. Was a match found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "character_2 = \"Henry\"\n",
    "\n",
    "# compile your regular expression here\n",
    "regular_expression = re.compile('[A-za-z]{7}')\n",
    "\n",
    "# check for a match to character_1 here\n",
    "\n",
    "result_1 = regular_expression.match(character_1)\n",
    "\n",
    "# store and print the matched text here\n",
    "\n",
    "match_1 =result_1.group(0)\n",
    "\n",
    "# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here\n",
    "\n",
    "result_2 =regular_expression.match(character_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching and Finding\n",
    "\n",
    "You can make your regular expression matches even more dynamic with the help of the .search() method. Unlike .match() which will only find matches at the start of a string, .search() will look left to right through an entire piece of text and return a match object for the first match to the regular expression given. If no match is found, .search() will return None. For example, to search for a sequence of 8 word characters in the string Are you a Munchkin?:\n",
    "\n",
    "    result = re.search(\"\\w{8}\",\"Are you a Munchkin?\")\n",
    "Using .search() on the string above will find a match of \"Munchkin\", while using .match() on the same string would return None!\n",
    "\n",
    "So far you have used methods that only return one piece of matching text. What if you want to find all the occurrences of a word or keyword in a piece of text to determine a frequency count? Step in the .findall() method!\n",
    "\n",
    "Given a regular expression as its first argument and a string as its second argument, .findall() will return a list of all non-overlapping matches of the regular expression in the string. Consider the below piece of text:\n",
    "\n",
    "    text = \"Everything is green here, while in the country of the Munchkins blue was the favorite color. But the people do not seem to be as friendly as the Munchkins, and I'm afraid we shall be unable to find a place to pass the night.\"\n",
    "To find all non-overlapping sequences of 8 word characters in the sentence you can do the following:\n",
    "\n",
    "    list_of_matches = re.findall(\"\\w{8}\",text)\n",
    "    \n",
    ".findall() will thus return the list ['Everythi', 'Munchkin', 'favorite', 'friendly', 'Munchkin']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# structions\n",
    "1.\n",
    "The entire text of L. Frank Baum’s The Wonderful Wizard of Oz has been stored in oz_text. .search() for the occurrence of \"wizard\" in oz_text. Store the result in found_wizard, and print it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Find all occurrences of \"lion\" in oz_text and store the result in all_lions. Print all_lions.\n",
    "\n",
    "\n",
    "3.\n",
    "Save the length of all_lions to number_lions and print it. Given the number of occurrences, is the word “lion” important to the text?\n",
    "\n",
    "It’s important to note that the number of words in an entire text can impact the importance of a given word’s frequency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(46, 52), match='wizard'>\n",
      "['lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion']\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# search oz_text for an occurrence of 'wizard' here\n",
    "found_wizard = re.search(\"wizard\",oz_text)\n",
    "print(found_wizard)\n",
    "\n",
    "# find all the occurrences of 'lion' in oz_text here\n",
    "all_lions = re.findall(\"lion\",oz_text)\n",
    "print(all_lions)\n",
    "\n",
    "# store and print the length of all_lions here\n",
    "number_lions = len(all_lions)\n",
    "print(number_lions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "While it is useful to match and search for patterns of individual characters in a text, you can often find more meaning by analyzing text on a word-by-word basis, focusing on the part of speech of each word in a sentence. This process of identifying and labeling the part of speech of words is known as part-of-speech tagging!\n",
    "\n",
    "It may have been a while since you’ve been in English class, so let’s review the nine parts of speech with an example:\n",
    "\n",
    "Wow! Ramona and her class are happily studying the new textbook she has on NLP.\n",
    "\n",
    "* Noun: the name of a person (Ramona,class), place, thing (textbook), or idea (NLP)\n",
    "\n",
    "* Pronoun: a word used in place of a noun (her,she)\n",
    "\n",
    "* Determiner: a word that introduces, or “determines”, a noun (the)\n",
    "\n",
    "* Verb: expresses action (studying) or being (are,has)\n",
    "\n",
    "* Adjective: modifies or describes a noun or pronoun (new)\n",
    "\n",
    "* Adverb: modifies or describes a verb, an adjective, or another adverb (happily)\n",
    "\n",
    "* Preposition: a word placed before a noun or pronoun to form a phrase modifying another word in the sentence (on)\n",
    "\n",
    "* Conjunction: a word that joins words, phrases, or clauses (and)\n",
    "\n",
    "* Interjection: a word used to express emotion (Wow)\n",
    "\n",
    "You can automate the part-of-speech tagging process with nltk‘s pos_tag() function! The function takes one argument, a list of words in the order they appear in a sentence, and returns a list of tuples, where the first entry in the tuple is a word and the second is the part-of-speech tag.\n",
    "\n",
    "Given the sentence split into a list of words below:\n",
    "\n",
    "    word_sentence = ['do', 'you', 'suppose', 'oz', 'could', 'give', 'me', 'a', 'heart', '?']\n",
    "    \n",
    "you can tag the parts of speech as follows:\n",
    "\n",
    "    part_of_speech_tagged_sentence = pos_tag(word_sentence)\n",
    "    \n",
    "The call to pos_tag() will return the following:\n",
    "\n",
    "    [('do', 'VB'), ('you', 'PRP'), ('suppose', 'VB'), ('oz', 'NNS'), ('could', 'MD'), ('give', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('heart', 'NN'), ('?', '.')]\n",
    "    \n",
    "Abbreviations are given instead of the full part of speech name. Some common abbreviations include: NN for nouns, VB for verbs, RB for adverbs, JJ for adjectives, and DT for determiners. A complete list of part-of-speech tags and their abbreviations can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1.\n",
    "Provided to you in the workspace is the text of The Wonderful Wizard of Oz, broken down into individual words on a sentence by sentence basis in a process known as tokenization. These sentences are called word tokenized sentences, which are stored in word_tokenized_oz.\n",
    "\n",
    "Save the value stored at index 100 of word_tokenized_oz to a variable named witches_fate, and print it. You should see a sentence from the novel, split into individual words, print to the terminal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Since the text has been broken down to individual words on a sentence by sentence level, you now can part-of-speech tag each word tokenized sentence in The Wonderful Wizard of Oz! Begin by creating an empty list named pos_tagged_oz to hold the part-of-speech tagged sentences.\n",
    "\n",
    "\n",
    "3.\n",
    "Create a for-loop through each word tokenized sentence in word_tokenized_oz. Within the for-loop, part-of-speech tag each word tokenized sentence and append the result to pos_tagged_oz.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Save the part-of-speech tagged sentence at index 100 to a variable named witches_fate_pos, and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'word_tokenized_oz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4aa11be50730>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mword_tokenized_oz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenized_oz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# save and print the sentence stored at index 100 in word_tokenized_oz here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'word_tokenized_oz'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from word_tokenized_oz import word_tokenized_oz\n",
    "\n",
    "# save and print the sentence stored at index 100 in word_tokenized_oz here\n",
    "witches_fate = word_tokenized_oz[100]\n",
    "print(witches_fate)\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_oz = list()\n",
    "\n",
    "# create a for loop through each word tokenized sentence in word_tokenized_oz here\n",
    "for word_tokenized_sentence in word_tokenized_oz:\n",
    "  # part-of-speech tag each sentence and append to pos_tagged_oz here\n",
    "  pos_tagged_oz.append(pos_tag(word_tokenized_sentence))\n",
    "\n",
    "# store and print the part-of-speech tagged sentence at index 100 in witches_fate_pos here\n",
    "witches_fate_pos = pos_tagged_oz[100]\n",
    "print(witches_fate_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Chunking\n",
    "\n",
    "You have made it to the juicy stuff! Given your part-of-speech tagged text, you can now use regular expressions to find patterns in sentence structure that give insight into the meaning of a text. This technique of grouping words by their part-of-speech tag is called chunking.\n",
    "\n",
    "With chunking in nltk, you can define a pattern of parts-of-speech tags using a modified notation of regular expressions. You can then find non-overlapping matches, or chunks of words, in the part-of-speech tagged sentences of a text.\n",
    "\n",
    "The regular expression you build to find chunks is called chunk grammar. A piece of chunk grammar can be written as follows:\n",
    "\n",
    "    chunk_grammar = \"AN: {<JJ><NN>}\"\n",
    "    \n",
    "AN is a user-defined name for the kind of chunk you are searching for. You can use whatever name makes sense given your chunk grammar. In this case AN stands for adjective-noun\n",
    "A pair of curly braces {} surround the actual chunk grammar\n",
    "<JJ> operates similarly to a regex character class, matching any adjective\n",
    "<NN> matches any noun, singular or plural\n",
    "The chunk grammar above will thus match any adjective that is followed by a noun.\n",
    "\n",
    "To use the chunk grammar defined, you must create a nltk RegexpParser object and give it a piece of chunk grammar as an argument.\n",
    "\n",
    "    chunk_parser = RegexpParser(chunk_grammar)\n",
    "    \n",
    "You can then use the RegexpParser object’s .parse() method, which takes a list of part-of-speech tagged words as an argument, and identifies where such chunks occur in the sentence!\n",
    "\n",
    "Consider the part-of-speech tagged sentence below:\n",
    "\n",
    "    pos_tagged_sentence = [('where', 'WRB'), ('is', 'VBZ'), ('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN'), ('?', '.')]\n",
    "    \n",
    "You can chunk the sentence to find any adjectives followed by a noun with the following:\n",
    "\n",
    "    chunked = chunk_parser.parse(pos_tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "Define a piece of chunk grammar named chunk_grammar that will chunk a single adjective followed by a single noun. Name the chunk AN.\n",
    "\n",
    "\n",
    "2.\n",
    "Create a RegexpParser object called chunk_parser using chunk_grammar as an argument.\n",
    "\n",
    "\n",
    "3.\n",
    "The part-of-speech tagged novel pos_tagged_oz from the previous exercise has been given to you in the workspace.\n",
    "\n",
    "Chunk the part-of-speech tagged sentence stored at index 282 in pos_tagged_oz using chunk_parser‘s .parse() method. Save the result to to a variable named scaredy_cat, and print it. The chunked sequences of an adjective followed by a noun will be indicated with an AN, the chunk name you defined earlier.\n",
    "\n",
    "\n",
    "4.\n",
    "nltk allows you to better visualize a chunked sentence with the .pretty_print() function. Uncomment the last line in the workspace and run the code to view the chunked sentence. Expand the output terminal all the way to the left to get a better view!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pos_tagged_oz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6631ae08abf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# define adjective-noun chunk grammar here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mchunk_grammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"AN:{<JJ><NN>}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pos_tagged_oz'"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser, Tree\n",
    "from pos_tagged_oz import pos_tagged_oz\n",
    "\n",
    "# define adjective-noun chunk grammar here\n",
    "chunk_grammar = \"AN:{<JJ><NN>}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# chunk the pos-tagged sentence at index 282 in pos_tagged_oz here\n",
    "scaredy_cat = chunk_parser.parse(pos_tagged_oz[282])\n",
    "print(scaredy_cat)\n",
    "\n",
    "# pretty_print the chunked sentence here\n",
    "Tree.fromstring(str(scaredy_cat)).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Noun Phrases\n",
    "\n",
    "While you are able to chunk any sequence of parts of speech that you like, there are certain types of chunking that are linguistically helpful for determining meaning and bias in a piece of text. One such type of chunking is NP-chunking, or noun phrase chunking. A noun phrase is a phrase that contains a noun and operates, as a unit, as a noun.\n",
    "\n",
    "A popular form of noun phrase begins with a determiner DT, which specifies the noun being referenced, followed by any number of adjectives JJ, which describe the noun, and ends with a noun NN.\n",
    "\n",
    "Consider the part-of-speech tagged sentence below:\n",
    "\n",
    "    [('we', 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('grateful', 'JJ'), ('to', 'TO'), ('you', 'PRP'), ('for', 'IN'), ('having', 'VBG'), ('killed', 'VBN'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('east', 'NN'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('setting', 'VBG'), ('our', 'PRP'), ('people', 'NNS'), ('free', 'VBP'), ('from', 'IN'), ('bondage', 'NN'), ('.', '.')]\n",
    "    \n",
    "Can you spot the three noun phrases of the form described above? They are:\n",
    "\n",
    "    (('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'))\n",
    "    (('the', 'DT'), ('east', 'NN'))\n",
    "    (('bondage', 'NN'))\n",
    "    \n",
    "With the help of a regular expression defined chunk grammar, you can easily find all the non-overlapping noun phrases in a piece of text! Just like in normal regular expressions, you can use quantifiers to indicate how many of each part of speech you want to match.\n",
    "\n",
    "The chunk grammar for a noun phrase can be written as follows:\n",
    "\n",
    "    chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    \n",
    "* NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun \n",
    "\n",
    "\n",
    "* DT matches any determiner\n",
    "    \n",
    "    \n",
    "* ? is an optional quantifier, matching either 0 or 1 determiners\n",
    "\n",
    "\n",
    "* <JJ> matches any adjective\n",
    "    \n",
    "    \n",
    "* (star or asteristic) is the Kleene star quantifier, matching 0 or more occurrences of an adjective\n",
    "\n",
    "\n",
    "* <NN> matches any noun, singular or plural\n",
    "    \n",
    "    \n",
    "By finding all the NP-chunks in a text, you can perform a frequency analysis and identify important, recurring noun phrases. You can also use these NP-chunks as pseudo-topics and tag articles and documents by their highest count NP-chunks! Or perhaps your analysis has you looking at the adjective choices an author makes for different nouns.\n",
    "\n",
    "It is ultimately up to you, with your knowledge of the text you are working with, to interpret the meaning and use-case of the NP-chunks and their frequency of occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1.\n",
    "Define a piece of chunk grammar named chunk_grammar that will chunk a noun phrase. Name the chunk NP.\n",
    "\n",
    "\n",
    "2.\n",
    "Create a RegexpParser object called chunk_parser using chunk_grammar as an argument.\n",
    "\n",
    "\n",
    "3.\n",
    "That part-of-speech tagged novel pos_tagged_oz you previously created has been imported for you in the workspace.\n",
    "\n",
    "Create a for loop through each part-of-speech tagged sentence in pos_tagged_oz. Within the for loop, NP-chunk each part-of-speech tagged sentence using chunk_parser‘s .parse() method and append the result to np_chunked_oz. Each item in np_chunked_oz will now be a noun phrase chunked sentence from The Wonderful Wizard of Oz!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pos_tagged_oz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b4f3a472f711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnp_chunk_counter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_chunk_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# define noun-phrase chunk grammar here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pos_tagged_oz'"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "from pos_tagged_oz import pos_tagged_oz\n",
    "from np_chunk_counter import np_chunk_counter\n",
    "\n",
    "# define noun-phrase chunk grammar here\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold noun-phrase chunked sentences\n",
    "np_chunked_oz = list()\n",
    "\n",
    "# create a for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to np_chunked_oz here\n",
    "  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the most common np-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Verb Phrases\n",
    "\n",
    "Another popular type of chunking is VP-chunking, or verb phrase chunking. A verb phrase is a phrase that contains a verb and its complements, objects, or modifiers.\n",
    "\n",
    "Verb phrases can take a variety of structures, and here you will consider two. The first structure begins with a verb VB of any tense, followed by a noun phrase, and ends with an optional adverb RB of any form. The second structure switches the order of the verb and the noun phrase, but also ends with an optional adverb.\n",
    "\n",
    "Both structures are considered because verb phrases of each form are essentially the same in meaning. For example, consider the part-of-speech tagged verb phrases given below:\n",
    "\n",
    "* (('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN'))\n",
    "\n",
    "* ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), (('said', 'VBD'),\n",
    "\n",
    "The chunk grammar to find the first form of verb phrase is given below:\n",
    "\n",
    "    chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"\n",
    "    \n",
    "* VP is the user-defined name of the chunk you are searching for. In this case VP stands for verb phrase\n",
    "\n",
    "* <VB.*> matches any verb using the . as a wildcard and the * quantifier to match 0 or more occurrences of any character. This ensures matching verbs of any tense (ex. VB for present tense, VBD for past tense, or VBN for past participle)\n",
    "\n",
    "* <DT>?<JJ>*<NN> matches any noun phrase\n",
    "    \n",
    "* <RB.?> matches any adverb using the . as a wildcard and the optional quantifier to match 0 or 1 occurrence of any character. This ensures matching any form of adverb (regular RB, comparative RBR, or superlative RBS)\n",
    "\n",
    "*  ? is an optional quantifier, matching either 0 or 1 adverbs\n",
    "\n",
    "The chunk grammar for the second form of verb phrase is given below:\n",
    "\n",
    "    chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "    \n",
    "Just like with NP-chunks, you can find all the VP-chunks in a text and perform a frequency analysis to identify important, recurring verb phrases. These verb phrases can give insight into what kind of action different characters take or how the actions that characters take are described by the author.\n",
    "\n",
    "Once again, this is the part of the analysis where you get to be creative and use your own knowledge about the text you are working with to find interesting insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pos_tagged_oz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4327d492ed5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mvp_chunk_counter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvp_chunk_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# define verb phrase chunk grammar here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pos_tagged_oz'"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "from pos_tagged_oz import pos_tagged_oz\n",
    "from vp_chunk_counter import vp_chunk_counter\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"\n",
    "#chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold verb-phrase chunked sentences\n",
    "vp_chunked_oz = list()\n",
    "\n",
    "# create for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to vp_chunked_oz here\n",
    "  vp_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "  \n",
    "# store and print the most common vp-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Filtering\n",
    "\n",
    "Another option you have to find chunks in your text is chunk filtering. Chunk filtering lets you define what parts of speech you do not want in a chunk and remove them.\n",
    "\n",
    "A popular method for performing chunk filtering is to chunk an entire sentence together and then indicate which parts of speech are to be filtered out. If the filtered parts of speech are in the middle of a chunk, it will split the chunk into two separate chunks! The chunk grammar you can use to perform chunk filtering is given below:\n",
    "\n",
    "    chunk_grammar = \"\"\"NP: {<.*>+}\n",
    "                           }<VB.?|IN>+{\"\"\"\n",
    "                           \n",
    "NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase\n",
    "\n",
    "\n",
    "The brackets {} indicate what parts of speech you are chunking. <.*>+ matches every part of speech in the sentence\n",
    "\n",
    "\n",
    "The inverted brackets }{ indicate which parts of speech you want to filter from the chunk. <VB.?|IN>+ will filter out any verbs or prepositions\n",
    "\n",
    "Chunk filtering provides an alternate way for you to search through a text and find the chunks of information useful for your analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1.\n",
    "The code in the workspace chunks an entire sentence together using the chunk grammar \"Chunk: {<.*>+}\". Run the code and view the output to see how the sentence is chunked into one big chunk named Chunk!\n",
    "\n",
    "\n",
    "2.\n",
    "Define a piece of chunk grammar named chunk_grammar that will chunk a noun phrase using chunk filtering. Name the chunk NP.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Create a RegexpParser object called chunk_parser using chunk_grammar as an argument.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Now you can find the NP-chunks in a sentence from The Wonderful Wizard of Oz using chunk filtering! Chunk and filter the part-of-speech tagged sentence stored at index 230 in pos_tagged_oz using chunk_parser‘s .parse() method. Save the result to filtered_dancers, and print filtered_dancers.\n",
    "\n",
    "What parts of speech are removed from the chunk? What chunks remain?\n",
    "\n",
    "\n",
    "5.\n",
    "The last line in the workspace .pretty_print()s the chunked and filtered sentence with nltk. Uncomment the line and run the code to view the chunked and filtered sentence. Expand the output terminal all the way to the left to get a better view!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pos_tagged_oz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3b761dd5bbb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# define chunk grammar to chunk an entire sentence together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Chunk: {<.*>+}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pos_tagged_oz'"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser, Tree\n",
    "from pos_tagged_oz import pos_tagged_oz\n",
    "\n",
    "# define chunk grammar to chunk an entire sentence together\n",
    "grammar = \"Chunk: {<.*>+}\"\n",
    "\n",
    "# create RegexpParser object\n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "# chunk the pos-tagged sentence at index 230 in pos_tagged_oz\n",
    "chunked_dancers = parser.parse(pos_tagged_oz[230])\n",
    "print(chunked_dancers)\n",
    "\n",
    "# define noun phrase chunk grammar using chunk filtering here\n",
    "chunk_grammar = \"\"\"NP: {<.*>+}\n",
    "                       }<VB.?|IN>+{\"\"\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# chunk and filter the pos-tagged sentence at index 230 in pos_tagged_oz here\n",
    "filtered_dancers = chunk_parser.parse(pos_tagged_oz[230])\n",
    "print(filtered_dancers)\n",
    "\n",
    "# pretty_print the chunked and filtered sentence here\n",
    "Tree.fromstring(str(filtered_dancers)).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "And there you go! Now you have the toolkit to dig into any piece of text data and perform natural language parsing with regular expressions. What insights will you gain, or what bias may you uncover? Let’s review what you have learned:\n",
    "\n",
    "* The re module’s .compile() and .match() methods allow you to enter any regex pattern and look for a single match at the beginning of a piece of text\n",
    "\n",
    "* The re module’s .search() method lets you find a single match to a regex pattern anywhere in a string, while the .findall() method finds all the matches of a regex pattern in a string\n",
    "\n",
    "* Part-of-speech tagging identifies and labels the part of speech of words in a sentence, and can be performed in nltk using the pos_tag() function\n",
    "\n",
    "* Chunking groups together patterns of words by their part-of-speech tag. Chunking can be performed in nltk by defining a piece of chunk grammar using regular expression syntax and calling a RegexpParser‘s .parse() method on a word tokenized sentence\n",
    "\n",
    "* NP-chunking chunks together an optional determiner DT, any number of adjectives JJ, and a noun NN to form a noun phrase. The frequency of different NP-chunks can identify important topics in a text or demonstrate how an author describes different subjects\n",
    "\n",
    "* VP-chunking chunks together a verb VB, a noun phrase, and an optional adverb RB to form a verb phrase. The frequency of different VP-chunks can give insight into what kind of action different subjects take or how the actions that different subjects take are described by an author, potentially indicating bias\n",
    "\n",
    "* Chunk filtering provides an alternative means of chunking by specifying what parts of speech you do not want in a chunk and removing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "The code in the workspace is set up to perform natural language parsing on The Wonderful Wizard of Oz. However, the chunk grammar is empty! Instead of finding NP-chunks or VP-chunks, define your own chunk grammar using regular expressions in between the curly braces {}. Feel free to add any chunk filtering in between the inverted braces }{ if you so desire!\n",
    "\n",
    "Run the code and observe the frequencies of the chunks. What insights or knowledge do the chunk frequencies give you? Have you come to any different conclusions than from analyzing the NP-chunks and VP-chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pos_tagged_oz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f6b08adc0487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_tagged_oz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunk_counter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchunk_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# define your own chunk grammar here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pos_tagged_oz'"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "from pos_tagged_oz import pos_tagged_oz\n",
    "from chunk_counter import chunk_counter\n",
    "\n",
    "# define your own chunk grammar here\n",
    "chunk_grammar = '''Chunk: {}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t}{'''\n",
    "\n",
    "# create RegexpParser object\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold chunked sentences\n",
    "chunked_oz = list()\n",
    "\n",
    "# create a for loop through each pos-tagged sentence in pos_tagged_oz\n",
    "for pos_tagged_sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to chunked_oz\n",
    "  chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the most common chunks\n",
    "most_common_chunks = chunk_counter(chunked_oz)\n",
    "print(most_common_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
