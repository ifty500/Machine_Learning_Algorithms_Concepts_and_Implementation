{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Bag-of-Words\n",
    "“A bag-of-words is all you need,” some NLPers have decreed.\n",
    "\n",
    "The bag-of-words language model is a simple-yet-powerful tool to have up your sleeve when working on natural language processing (NLP). The model has many, many use cases including:\n",
    "\n",
    "* determining topics in a song\n",
    "\n",
    "* filtering spam from your inbox\n",
    "\n",
    "* finding out if a tweet has positive or negative sentiment\n",
    "\n",
    "* creating word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from preprocessing import preprocess_text\n",
    "\n",
    "# Add your email text to test_text between the triple quotes:\n",
    "test_text = \"\"\"\n",
    "Our records indicate your Pension is under-performing to see higher growth and up to 25% cash release reply for a free review.\n",
    "\"\"\"\n",
    "test_text1 = \"\"\"\n",
    "Have you ever wondered how an email ends up in the spam folder? Or how customer service phone systems are able to understand what you're saying? From a cleaner inbox to faster customer service and virtual assistants that can tell you the weather, the number of applications using natural language processing (NLP) is rapidly growing. NLP is all about how computers work with human language. Learn how to create your own powerful NLP programs with this new course!\n",
    "\n",
    "Start Now\n",
    "\n",
    "Happy coding,\n",
    "Codecademy\n",
    "\"\"\"\n",
    "test_tokens = preprocess_text(test_text)\n",
    "\n",
    "def create_features_dictionary(document_tokens):\n",
    "  features_dictionary = {}\n",
    "  index = 0\n",
    "  for token in document_tokens:\n",
    "    if token not in features_dictionary:\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "  return features_dictionary\n",
    "\n",
    "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
    "  bow_vector = [0] * len(features_dictionary)\n",
    "  for token in document_tokens:\n",
    "    if token in features_dictionary:\n",
    "      feature_index = features_dictionary[token]\n",
    "      bow_vector[feature_index] += 1\n",
    "  return bow_vector\n",
    "\n",
    "bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n",
    "training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]\n",
    "test_vectors = [tokens_to_bow_vector(test_tokens, bow_sms_dictionary)]\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.predict(test_vectors)\n",
    "\n",
    "print(\"Looks like a normal email!\" if predictions[0] == 0 else \"You've got spam!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-What?\n",
    "\n",
    "Bag-of-words (BoW) is a statistical language model based on word count. Say what?\n",
    "\n",
    "Let’s start with that first part: a statistical language model is a way for computers to make sense of language based on probability. For example, let’s say we have the text:\n",
    "\n",
    "“Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?”\n",
    "\n",
    "A statistical language model focused on the starting letter for words might take this text and predict that words are most likely to start with the letter “f” because 11 out of 15 words begin that way. A different statistical model that pays attention to word order might tell us that the word “fish” tends to follow the word “fantastic.”\n",
    "\n",
    "Bag-of-words does not give a flying fish about word starts or word order though; its sole concern is word count — how many times each word appears in a document.\n",
    "\n",
    "If you’re already familiar with statistical language models, you may also have heard BoW referred to as the unigram model. It’s technically a special case of another statistical model, the n-gram model, with n (the number of words in a sequence) set to 1.\n",
    "\n",
    "If you have no idea what n-grams are, don’t worry — we’ll dive deeper into them in another lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Dictionaries\n",
    "\n",
    "One of the most common ways to implement the BoW model in Python is as a dictionary with each key set to a word and each value set to the number of times that word appears. Take the example below:\n",
    "\n",
    "The squids jumped out of the suitcases.\n",
    "The words from the sentence go into the bag-of-words and come out as a dictionary of words with their corresponding counts. For statistical models, we call the text that we use to build the model our training data. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences).\n",
    "\n",
    "Let’s build a function that converts a given training text into a bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "Define a function text_to_bow() that accepts some_text as a variable. Inside the function, set bow_dictionary equal to an empty dictionary and return it from the function. This is where we’ll be collecting the words and their counts.\n",
    "\n",
    "\n",
    "2.\n",
    "Above the return statement, call the preprocess_text() function we created for you on some_text and assign the result to the variable tokens.\n",
    "\n",
    "Text preprocessing allows us to count words like “game” and “Games” as the same word token.\n",
    "\n",
    "\n",
    "3.\n",
    "Still above the return, iterate over each token in tokens and check if token is already in the bow_dictionary.\n",
    "\n",
    "If it is, increment that token’s count by 1. (Remember that each token‘s count is its corresponding value within the bow_dictionary.)\n",
    "Otherwise, set the count equal to 1 because this is the first time the model has seen that word token.\n",
    "\n",
    "\n",
    "4.\n",
    "Uncomment the print statement and run the code to see your bag-of-words function in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 2, 'love': 1, 'fantastic': 2, 'fly': 2, 'fish': 3, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1}\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing import preprocess_text\n",
    "# Define text_to_bow() below:\n",
    "def text_to_bow(some_text):\n",
    "  bow_dictionary = {}\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    if token in bow_dictionary:\n",
    "      bow_dictionary[token] += 1\n",
    "    else:\n",
    "      bow_dictionary[token] = 1\n",
    "  return bow_dictionary\n",
    "\n",
    "print(text_to_bow(\"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing BoW Vectors\n",
    "\n",
    "Sometimes a dictionary just won’t fit the bill. Topic modelling applications, for example, require an implementation of bag-of-words that is a bit more mathematical: feature vectors.\n",
    "\n",
    "A feature vector is a numeric representation of an item’s important features. Each feature has its own column. If the feature exists for the item, you could represent that with a 1. If the feature does not exist for that item, you could represent that with a 0. A few monsters could be represented as vectors like so:\n",
    "\n",
    "|has_fangs|\tmelts_in_water|\thates_sunlight|\thas_fur|\n",
    "|---|---|---|---|\n",
    "|vampire|\t1|\t0|\t1|\t0|\n",
    "|werewolf|\t1|\t0|\t0|\t1|\n",
    "|witch|\t0|\t1|\t0|\t0|\n",
    "\n",
    "For bag-of-words, instead of monsters you would have documents and the features would be different words. And we don’t just care if a word is present in a document; we want to know how many times it occurred! Turning text into a BoW vector is known as feature extraction or vectorization.\n",
    "\n",
    "But how do we know which vector index corresponds to which word? When building BoW vectors, we generally create a features dictionary of all vocabulary in our training data (usually several documents) mapped to indices.\n",
    "\n",
    "For example, with “Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?” our dictionary might be:\n",
    "\n",
    "    {'five': 0,\n",
    "    'fantastic': 1,\n",
    "    'fish': 2,\n",
    "    'fly': 3,\n",
    "    'off': 4,\n",
    "    'to': 5,\n",
    "    'find': 6,\n",
    "    'faraway': 7,\n",
    "    'function': 8,\n",
    "    'maybe': 9,\n",
    "    'another': 10}\n",
    "    \n",
    "Using this dictionary, we can convert new documents into vectors using a vectorization function. For example, we can take a brand new sentence “Another five fish find another faraway fish.” — test data — and convert it to a vector that looks like:\n",
    "\n",
    "    [1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2]\n",
    "    \n",
    "The word ‘another’ appeared twice in the test data. If we look at the feature dictionary for ‘another’, we find that its index is 10. So when we go back and look at our vector, we’d expect the number at index 10 to be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
