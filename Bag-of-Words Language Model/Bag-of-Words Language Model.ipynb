{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Bag-of-Words\n",
    "“A bag-of-words is all you need,” some NLPers have decreed.\n",
    "\n",
    "The bag-of-words language model is a simple-yet-powerful tool to have up your sleeve when working on natural language processing (NLP). The model has many, many use cases including:\n",
    "\n",
    "* determining topics in a song\n",
    "\n",
    "* filtering spam from your inbox\n",
    "\n",
    "* finding out if a tweet has positive or negative sentiment\n",
    "\n",
    "* creating word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from preprocessing import preprocess_text\n",
    "\n",
    "# Add your email text to test_text between the triple quotes:\n",
    "test_text = \"\"\"\n",
    "Our records indicate your Pension is under-performing to see higher growth and up to 25% cash release reply for a free review.\n",
    "\"\"\"\n",
    "test_text1 = \"\"\"\n",
    "Have you ever wondered how an email ends up in the spam folder? Or how customer service phone systems are able to understand what you're saying? From a cleaner inbox to faster customer service and virtual assistants that can tell you the weather, the number of applications using natural language processing (NLP) is rapidly growing. NLP is all about how computers work with human language. Learn how to create your own powerful NLP programs with this new course!\n",
    "\n",
    "Start Now\n",
    "\n",
    "Happy coding,\n",
    "Codecademy\n",
    "\"\"\"\n",
    "test_tokens = preprocess_text(test_text)\n",
    "\n",
    "def create_features_dictionary(document_tokens):\n",
    "  features_dictionary = {}\n",
    "  index = 0\n",
    "  for token in document_tokens:\n",
    "    if token not in features_dictionary:\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "  return features_dictionary\n",
    "\n",
    "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
    "  bow_vector = [0] * len(features_dictionary)\n",
    "  for token in document_tokens:\n",
    "    if token in features_dictionary:\n",
    "      feature_index = features_dictionary[token]\n",
    "      bow_vector[feature_index] += 1\n",
    "  return bow_vector\n",
    "\n",
    "bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n",
    "training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]\n",
    "test_vectors = [tokens_to_bow_vector(test_tokens, bow_sms_dictionary)]\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.predict(test_vectors)\n",
    "\n",
    "print(\"Looks like a normal email!\" if predictions[0] == 0 else \"You've got spam!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-What?\n",
    "\n",
    "Bag-of-words (BoW) is a statistical language model based on word count. Say what?\n",
    "\n",
    "Let’s start with that first part: a statistical language model is a way for computers to make sense of language based on probability. For example, let’s say we have the text:\n",
    "\n",
    "“Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?”\n",
    "\n",
    "A statistical language model focused on the starting letter for words might take this text and predict that words are most likely to start with the letter “f” because 11 out of 15 words begin that way. A different statistical model that pays attention to word order might tell us that the word “fish” tends to follow the word “fantastic.”\n",
    "\n",
    "Bag-of-words does not give a flying fish about word starts or word order though; its sole concern is word count — how many times each word appears in a document.\n",
    "\n",
    "If you’re already familiar with statistical language models, you may also have heard BoW referred to as the unigram model. It’s technically a special case of another statistical model, the n-gram model, with n (the number of words in a sequence) set to 1.\n",
    "\n",
    "If you have no idea what n-grams are, don’t worry — we’ll dive deeper into them in another lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Dictionaries\n",
    "\n",
    "One of the most common ways to implement the BoW model in Python is as a dictionary with each key set to a word and each value set to the number of times that word appears. Take the example below:\n",
    "\n",
    "<img src=\"images/bag-of-words.gif\" width=\"500\" height=\"400\">\n",
    "\n",
    "The squids jumped out of the suitcases.\n",
    "The words from the sentence go into the bag-of-words and come out as a dictionary of words with their corresponding counts. For statistical models, we call the text that we use to build the model our training data. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences).\n",
    "\n",
    "Let’s build a function that converts a given training text into a bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "Define a function text_to_bow() that accepts some_text as a variable. Inside the function, set bow_dictionary equal to an empty dictionary and return it from the function. This is where we’ll be collecting the words and their counts.\n",
    "\n",
    "\n",
    "2.\n",
    "Above the return statement, call the preprocess_text() function we created for you on some_text and assign the result to the variable tokens.\n",
    "\n",
    "Text preprocessing allows us to count words like “game” and “Games” as the same word token.\n",
    "\n",
    "\n",
    "3.\n",
    "Still above the return, iterate over each token in tokens and check if token is already in the bow_dictionary.\n",
    "\n",
    "If it is, increment that token’s count by 1. (Remember that each token‘s count is its corresponding value within the bow_dictionary.)\n",
    "Otherwise, set the count equal to 1 because this is the first time the model has seen that word token.\n",
    "\n",
    "\n",
    "4.\n",
    "Uncomment the print statement and run the code to see your bag-of-words function in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 2, 'love': 1, 'fantastic': 2, 'fly': 2, 'fish': 3, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1}\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing import preprocess_text\n",
    "# Define text_to_bow() below:\n",
    "def text_to_bow(some_text):\n",
    "  bow_dictionary = {}\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    if token in bow_dictionary:\n",
    "      bow_dictionary[token] += 1\n",
    "    else:\n",
    "      bow_dictionary[token] = 1\n",
    "  return bow_dictionary\n",
    "\n",
    "print(text_to_bow(\"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing BoW Vectors\n",
    "\n",
    "Sometimes a dictionary just won’t fit the bill. Topic modelling applications, for example, require an implementation of bag-of-words that is a bit more mathematical: feature vectors.\n",
    "\n",
    "A feature vector is a numeric representation of an item’s important features. Each feature has its own column. If the feature exists for the item, you could represent that with a 1. If the feature does not exist for that item, you could represent that with a 0. A few monsters could be represented as vectors like so:\n",
    "\n",
    "|has_fangs|\tmelts_in_water|\thates_sunlight|\thas_fur|\n",
    "|---|---|---|---|\n",
    "|vampire|\t1|\t0|\t1|\t0|\n",
    "|werewolf|\t1|\t0|\t0|\t1|\n",
    "|witch|\t0|\t1|\t0|\t0|\n",
    "\n",
    "For bag-of-words, instead of monsters you would have documents and the features would be different words. And we don’t just care if a word is present in a document; we want to know how many times it occurred! Turning text into a BoW vector is known as feature extraction or vectorization.\n",
    "\n",
    "But how do we know which vector index corresponds to which word? When building BoW vectors, we generally create a features dictionary of all vocabulary in our training data (usually several documents) mapped to indices.\n",
    "\n",
    "For example, with “Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?” our dictionary might be:\n",
    "\n",
    "    {'five': 0,\n",
    "    'fantastic': 1,\n",
    "    'fish': 2,\n",
    "    'fly': 3,\n",
    "    'off': 4,\n",
    "    'to': 5,\n",
    "    'find': 6,\n",
    "    'faraway': 7,\n",
    "    'function': 8,\n",
    "    'maybe': 9,\n",
    "    'another': 10}\n",
    "    \n",
    "Using this dictionary, we can convert new documents into vectors using a vectorization function. For example, we can take a brand new sentence “Another five fish find another faraway fish.” — test data — and convert it to a vector that looks like:\n",
    "\n",
    "    [1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2]\n",
    "    \n",
    "The word ‘another’ appeared twice in the test data. If we look at the feature dictionary for ‘another’, we find that its index is 10. So when we go back and look at our vector, we’d expect the number at index 10 to be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Features Dictionary\n",
    "Now that you know what a bag-of-words vector looks like, you can create a function that builds them!\n",
    "\n",
    "First, we need a way of generating a features dictionary from a list of training documents. We can build a Python function to do that for us…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "Define a function create_features_dictionary() that takes one argument, documents. This will be the list of string documents that we pass in (like [\"All the cool fish love to fly high.\", \"Nobody knows why the fish fly so high.\", \"Those cool fish sure are spry.\"]).\n",
    "\n",
    "Inside the function, set features_dictionary equal to an empty dictionary. This is where we’ll map all of our terms to index numbers. For now, return features_dictionary from the function.\n",
    "\n",
    "\n",
    "2.\n",
    "Above the return statement, merge the documents into a string joined together by spaces and assign the result to merged.\n",
    "\n",
    "Now that the documents are all in a single string, call preprocess_text() on merged and assign the result to tokens. Return tokens from the function in addition to features_dictionary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Above the return statement, assign index a value of 0. This will correspond to the first word’s vector index.\n",
    "\n",
    "\n",
    "4.\n",
    "The words are prepared, the empty dictionary is prepared, and we have an index number we can use; it’s time to get the words into the dictionary and link each to a vector index number!\n",
    "\n",
    "Above the return, loop through each token in tokens.\n",
    "In the loop, check if token is NOT in features_dictionary.\n",
    "If it’s a new word, add token as a key to features_dictionary with a value of index.\n",
    "\n",
    "\n",
    "5.\n",
    "After adding token to features_dictionary, increment index by 1 so that each new word has its own index.\n",
    "\n",
    "\n",
    "6.\n",
    "Uncomment the print statement to test out the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing import preprocess_text\n",
    "# Define create_features_dictionary() below:\n",
    "def create_features_dictionary(documents):\n",
    "  features_dictionary = {}\n",
    "  merged = \" \".join(documents)\n",
    "  tokens = preprocess_text(merged)\n",
    "  index = 0\n",
    "  for token in tokens:\n",
    "    if token not in features_dictionary:\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "  return features_dictionary, tokens\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "\n",
    "print(create_features_dictionary(training_documents)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a BoW Vector\n",
    "\n",
    "Nice work! Time to put that dictionary of vocabulary to good use and build a bag-of-words vector from a new document.\n",
    "\n",
    "In Python, we can use a list to represent a vector. Each index in the list will correspond to a word and be set to its count.\n",
    "\n",
    "<img src=\"images/Building_vector.gif\" width=\"500\" height=\"400\">\n",
    "\n",
    "features dictionary of words 'all, my, fish, fly, away, help, me' transforms the string 'help my fly fish fly away' into the vector [0,1,1,2,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1.\n",
    "Define a function text_to_bow_vector() with two parameters:\n",
    "\n",
    "some_text (the document we pass in to vectorize)\n",
    "features_dictionary (the dictionary of vocabulary we generated in the previous exercise)\n",
    "Create a list of 0s the length of features_dictionary and assign it to the variable bow_vector. Each 0 represents a word’s count within the vector.\n",
    "\n",
    "Return bow_vector from the function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Above the return statement, preprocess the some_text document using the preprocess_text() function we built for you and assign the result to the variable tokens. Add tokens as a second return value for the function.\n",
    "\n",
    "\n",
    "3.\n",
    "Still above the return statement, loop through each token in tokens.\n",
    "\n",
    "Determine which index the token has within features_dictionary and assign the value to a new variable feature_index. (Take a look a the gif. If token is the word fish, then we would want feature_index to be 2.)\n",
    "Now that you have the word’s index, access the word count index within the bow_vector and increment that count by 1.\n",
    "\n",
    "\n",
    "4.\n",
    "Uncomment the print statement to test out the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing import preprocess_text\n",
    "# Define text_to_bow_vector() below:\n",
    "def text_to_bow_vector(some_text, features_dictionary):\n",
    "  bow_vector = [0] * len(features_dictionary)\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    feature_index = features_dictionary[token]\n",
    "    bow_vector[feature_index] += 1\n",
    "  return bow_vector, tokens\n",
    "\n",
    "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(text_to_bow_vector(text, features_dictionary)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's All in the Bag\n",
    "Phew! That was a lot of work.\n",
    "\n",
    "It’s time to put create_features_dictionary() and tokens_to_bow_vector() together and use them in a spam filter we created that uses a Naive Bayes classifier. We’ve slightly modified the two functions for this use case, but they should still look familiar.\n",
    "\n",
    "Let’s see create_features_dictionary() and tokens_to_bow_vector() in action with real test data, helping fend off spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "Below tokens_to_bow_vector(), call create_features_dictionary() on training_doc_tokens and assign the result to bow_sms_dictionary.\n",
    "\n",
    "\n",
    "2.\n",
    "Define training_vectors as a list comprehension. The list comprehension should call tokens_to_bow_vector() on training_doc and bow_sms_dictionary for each training_doc in training_spam_docs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Define test_vectors as a list comprehension that calls tokens_to_bow_vector() on test_doc and bow_sms_dictionary for each test_doc in test_spam_docs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Ready? Get set! Uncomment the code at the bottom of script.py and run the code again!\n",
    "\n",
    "The Naive Bayes classifier was pretty darn accurate in determining which messages were spam by using your bag-of-words functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spam_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e4b9409cad43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspam_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_spam_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_doc_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_spam_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_features_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mfeatures_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spam_data'"
     ]
    }
   ],
   "source": [
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def create_features_dictionary(document_tokens):\n",
    "  features_dictionary = {}\n",
    "  index = 0\n",
    "  for token in document_tokens:\n",
    "    if token not in features_dictionary:\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "  return features_dictionary\n",
    "\n",
    "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
    "  bow_vector = [0] * len(features_dictionary)\n",
    "  for token in document_tokens:\n",
    "    if token in features_dictionary:\n",
    "      feature_index = features_dictionary[token]\n",
    "      bow_vector[feature_index] += 1\n",
    "  return bow_vector\n",
    "\n",
    "# Define bow_sms_dictionary:\n",
    "bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n",
    "\n",
    "# Define training_vectors:\n",
    "training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]\n",
    "\n",
    "# Define test_vectors:\n",
    "test_vectors = [tokens_to_bow_vector(test_doc, bow_sms_dictionary) for test_doc in test_spam_docs]\n",
    "\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "\n",
    "def spam_or_not(label):\n",
    "  return \"spam\" if label else \"not spam\"\n",
    "\n",
    "# Uncomment the code below when you're done:\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.score(test_vectors, test_labels)\n",
    "\n",
    "print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100, test_docs[0], spam_or_not(test_labels[0]), test_docs[10], spam_or_not(test_labels[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam A Lot No More\n",
    "Amazing work! As is the case with many tasks in Python, there’s already a library that can do all of that work for you.\n",
    "\n",
    "For text_to_bow(), you can approximate the functionality with the collections module’s Counter() function:\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']\n",
    "    print(Counter(tokens))\n",
    "\n",
    "    # Counter({'fish': 2, 'another': 2, 'find': 1, 'five': 1, 'faraway': 1})\n",
    "For vectorization, you can use CountVectorizer from the machine learning library scikit-learn. You can use fit() to train the features dictionary and then transform() to transform text into a vector:\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "    \n",
    "    test_text = [\"Another five fish find another faraway fish.\"]\n",
    "    \n",
    "    bow_vectorizer = CountVectorizer()\n",
    "    \n",
    "    bow_vectorizer.fit(training_documents)\n",
    "    \n",
    "    bow_vector = bow_vectorizer.transform(test_text)\n",
    "    \n",
    "    print(bow_vector.toarray())\n",
    "    \n",
    "    # [[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "Now, let’s see how scikit-learn stacks up with the same bag-of-words functionality! Import CountVectorizer from sklearn. (Check out the example we gave for how to import CountVectorizer.)\n",
    "\n",
    "\n",
    "2.\n",
    "Define bow_vectorizer as our vectorizer using CountVectorizer().\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Define training_vectors as bow_vectorizer.fit_transform() called on training_docs.\n",
    "\n",
    "fit_transform() does two things: creation of the features dictionary and the vectorization of the training data.\n",
    "\n",
    "Define test_vectors as bow_vectorizer.transform() called on test_docs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Uncomment the code at the bottom of script.py. Run the code again to see why it makes sense to use sklearn‘s optimized functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a9a2871fd202>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Define training_vectors:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtraining_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# Define test_vectors:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtest_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_docs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import CountVectorizer from sklearn:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define bow_vectorizer:\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Define training_vectors:\n",
    "training_vectors = bow_vectorizer.fit_transform(training_docs)\n",
    "# Define test_vectors:\n",
    "test_vectors = bow_vectorizer.transform(test_docs)\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "\n",
    "def spam_or_not(label):\n",
    "  return \"spam\" if label else \"not spam\"\n",
    "\n",
    "# Uncomment the code below when you're done:\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.score(test_vectors, test_labels)\n",
    "\n",
    "print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100, test_docs[7], spam_or_not(test_labels[7]), test_docs[15], spam_or_not(test_labels[15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Wow\n",
    "\n",
    "As you can see, bag-of-words is pretty useful! BoW also has several advantages over other language models. For one, it’s an easier model to get started with and a few Python libraries already have built-in support for it.\n",
    "\n",
    "Because bag-of-words relies on single words, rather than sequences of words, there are more examples of each unit of language in the training corpus. More examples means the model has less data sparsity (i.e., it has more training knowledge to draw from) than other statistical models.\n",
    "\n",
    "Imagine you want to make a shirt to sell to people. If you have the shirt exactly tailored to someone’s body, it probably won’t fit that many people. But if you make a shirt that is just a giant bag with arm holes, you know that no one will buy it. What do you do? You loosely fit the shirt to someone’s body, leaving some extra room for different body shapes.\n",
    "\n",
    "Overfitting (adapting a model too strongly to training data, akin to our highly tailored shirt) is a common problem for statistical language models. While BoW still suffers from overfitting in terms of vocabulary, it overfits less than other statistical models, allowing for more flexibility in grammar and word choice.\n",
    "\n",
    "The combination of low data sparsity and less overfitting makes the bag-of-words model more reliable with smaller training data sets than other statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "The bigrams model is another statistical model that is helpful for tasks like text prediction. However, it’s not always ideal when you want to determine the topic of a given text.\n",
    "\n",
    "Read the short text in the code and then run the code as is to see the most common bigrams.\n",
    "\n",
    "\n",
    "2.\n",
    "Because of data sparsity, each bigram only has a single occurrence. As a result, the bigram model alone is not great at making predictions about topic or sentiment. Let’s see how the bag-of-words model does…\n",
    "\n",
    "Define bag_of_words in the code editor as Counter() called on tokens.\n",
    "\n",
    "3.\n",
    "At the end of script.py, let’s print out the three most frequently occurring words in the text. You can find the most common words by calling the Counter method .most_common() on bag_of_words and passing in a number — in this case 3 — as an argument. Save the result to most_common_three and print the result.\n",
    "\n",
    "Can you tell what the topic is by looking at the bag of words model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three most frequent word sequences and the number of occurrences according to Bigrams:\n",
      "[(('it', 's'), 1), (('s', 'excite'), 1), (('excite', 'to'), 1)]\n",
      "\n",
      "Three most frequent words and number of occurrences according to Bag of Words:\n",
      "[('fish', 4), ('fly', 3), ('day', 3)]\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing import preprocess_text\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"It's exciting to watch flying fish after a hard day's work. I don't know why some fish prefer flying and other fish would rather swim. It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'\"\n",
    "tokens = preprocess_text(text)\n",
    "\n",
    "# Bigram approach:\n",
    "bigrams_prepped = ngrams(tokens, 2)\n",
    "bigrams = Counter(bigrams_prepped)\n",
    "print(\"Three most frequent word sequences and the number of occurrences according to Bigrams:\")\n",
    "print(bigrams.most_common(3))\n",
    "\n",
    "# Bag of Words approach:\n",
    "# Define bag_of_words here:\n",
    "bag_of_words = Counter(tokens)\n",
    "print(\"\\nThree most frequent words and number of occurrences according to Bag of Words:\")\n",
    "most_common_three = bag_of_words.most_common(3)\n",
    "print(most_common_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Ow\n",
    "Alas, there is a trade-off for all the brilliance BoW brings to the table.\n",
    "\n",
    "Unless you want sentences that look like “the a but for the”, BoW is NOT a great primary model for text prediction. If that sort of “sentence” isn’t your bag, it’s because bag-of-words has high perplexity, meaning that it’s not a very accurate model for language prediction. The probability of the following word is always just the most frequently used words.\n",
    "\n",
    "If your BoW model finds “good” frequently occurring in a text sample, you might assume there’s a positive sentiment being communicated in that text… but if you look at the original text you may find that in fact every “good” was preceded by a “not.”\n",
    "\n",
    "Hmm, that would have been helpful to know. The BoW model’s word tokens lack context, which can make a word’s intended meaning unclear.\n",
    "\n",
    "Perhaps you are wondering, “What happens if the model comes across a new word that wasn’t in the training data?” As mentioned, like all statistical models, BoW suffers from overfitting when it comes to vocabulary.\n",
    "\n",
    "There are several ways that NLP developers have tackled this issue. A common approach is through language smoothing in which some probability is siphoned from the known words and given to unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscar_wilde_thoughts = \"\"\"\n",
    "IN my last lecture I gave you something of the history of Art in England.\n",
    "I sought to trace the influence of the French Revolution upon its\n",
    "development.  I said something of the song of Keats and the school of the\n",
    "pre-Raphaelites.  But I do not want to shelter the movement, which I have\n",
    "called the English Renaissance, under any palladium however noble, or any\n",
    "name however revered.  The roots of it have, indeed, to be sought for in\n",
    "things that have long passed away, and not, as some suppose, in the fancy\n",
    "of a few young menalthough I am not altogether sure that there is\n",
    "anything much better than the fancy of a few young men.\n",
    "\n",
    "When I appeared before you on a previous occasion, I had seen nothing of\n",
    "American art save the Doric columns and Corinthian chimney-pots visible\n",
    "on your Broadway and Fifth Avenue.  Since then, I have been through your\n",
    "country to some fifty or sixty different cities, I think.  I find that\n",
    "what your people need is not so much high imaginative art but that which\n",
    "hallows the vessels of everyday use.  I suppose that the poet will sing\n",
    "and the artist will paint regardless whether the world praises or blames.\n",
    "He has his own world and is independent of his fellow-men.  But the\n",
    "handicraftsman is dependent on your pleasure and opinion.  He needs your\n",
    "encouragement and he must have beautiful surroundings.  Your people love\n",
    "art but do not sufficiently honour the handicraftsman.  Of course, those\n",
    "millionaires who can pillage Europe for their pleasure need have no care\n",
    "to encourage such; but I speak for those whose desire for beautiful\n",
    "things is larger than their means.  I find that one great trouble all\n",
    "over is that your workmen are not given to noble designs.  You cannot be\n",
    "indifferent to this, because Art is not something which you can take or\n",
    "leave.  It is a necessity of human life.\n",
    "\n",
    "And what is the meaning of this beautiful decoration which we call art?\n",
    "In the first place, it means value to the workman and it means the\n",
    "pleasure which he must necessarily take in making a beautiful thing.  The\n",
    "mark of all good art is not that the thing done is done exactly or\n",
    "finely, for machinery may do as much, but that it is worked out with the\n",
    "head and the workmans heart.  I cannot impress the point too frequently\n",
    "that beautiful and rational designs are necessary in all work.  I did not\n",
    "imagine, until I went into some of your simpler cities, that there was so\n",
    "much bad work done.  I found, where I went, bad wall-papers horribly\n",
    "designed, and coloured carpets, and that old offender the horse-hair\n",
    "sofa, whose stolid look of indifference is always so depressing.  I found\n",
    "meaningless chandeliers and machine-made furniture, generally of\n",
    "rosewood, which creaked dismally under the weight of the ubiquitous\n",
    "interviewer.  I came across the small iron stove which they always\n",
    "persist in decorating with machine-made ornaments, and which is as great\n",
    "a bore as a wet day or any other particularly dreadful institution.  When\n",
    "unusual extravagance was indulged in, it was garnished with two funeral\n",
    "urns.\n",
    "\n",
    "It must always be remembered that what is well and carefully made by an\n",
    "honest workman, after a rational design, increases in beauty and value as\n",
    "the years go on.  The old furniture brought over by the Pilgrims, two\n",
    "hundred years ago, which I saw in New England, is just as good and as\n",
    "beautiful to-day as it was when it first came here.  Now, what you must\n",
    "do is to bring artists and handicraftsmen together.  Handicraftsmen\n",
    "cannot live, certainly cannot thrive, without such companionship.\n",
    "Separate these two and you rob art of all spiritual motive.\n",
    "\n",
    "Having done this, you must place your workman in the midst of beautiful\n",
    "surroundings.  The artist is not dependent on the visible and the\n",
    "tangible.  He has his visions and his dreams to feed on.  But the workman\n",
    "must see lovely forms as he goes to his work in the morning and returns\n",
    "at eventide.  And, in connection with this, I want to assure you that\n",
    "noble and beautiful designs are never the result of idle fancy or\n",
    "purposeless day-dreaming.  They come only as the accumulation of habits\n",
    "of long and delightful observation.  And yet such things may not be\n",
    "taught.  Right ideas concerning them can certainly be obtained only by\n",
    "those who have been accustomed to rooms that are beautiful and colours\n",
    "that are satisfying.\n",
    "\n",
    "Perhaps one of the most difficult things for us to do is to choose a\n",
    "notable and joyous dress for men.  There would be more joy in life if we\n",
    "were to accustom ourselves to use all the beautiful colours we can in\n",
    "fashioning our own clothes.  The dress of the future, I think, will use\n",
    "drapery to a great extent and will abound with joyous colour.  At present\n",
    "we have lost all nobility of dress and, in doing so, have almost\n",
    "annihilated the modern sculptor.  And, in looking around at the figures\n",
    "which adorn our parks, one could almost wish that we had completely\n",
    "killed the noble art.  To see the frock-coat of the drawing-room done in\n",
    "bronze, or the double waistcoat perpetuated in marble, adds a new horror\n",
    "to death.  But indeed, in looking through the history of costume, seeking\n",
    "an answer to the questions we have propounded, there is little that is\n",
    "either beautiful or appropriate.  One of the earliest forms is the Greek\n",
    "drapery which is exquisite for young girls.  And then, I think we may be\n",
    "pardoned a little enthusiasm over the dress of the time of Charles I., so\n",
    "beautiful indeed, that in spite of its invention being with the Cavaliers\n",
    "it was copied by the Puritans.  And the dress for the children of that\n",
    "time must not be passed over.  It was a very golden age of the little\n",
    "ones.  I do not think that they have ever looked so lovely as they do in\n",
    "the pictures of that time.  The dress of the last century in England is\n",
    "also peculiarly gracious and graceful.  There is nothing bizarre or\n",
    "strange about it, but it is full of harmony and beauty.  In these days,\n",
    "when we have suffered dreadfully from the incursions of the modern\n",
    "milliner, we hear ladies boast that they do not wear a dress more than\n",
    "once.  In the old days, when the dresses were decorated with beautiful\n",
    "designs and worked with exquisite embroidery, ladies rather took a pride\n",
    "in bringing out the garment and wearing it many times and handing it down\n",
    "to their daughtersa process that would, I think, be quite appreciated by\n",
    "a modern husband when called upon to settle his wifes bills.\n",
    "\n",
    "And how shall men dress?  Men say that they do not particularly care how\n",
    "they dress, and that it is little matter.  I am bound to reply that I do\n",
    "not think that you do.  In all my journeys through the country, the only\n",
    "well-dressed men that I sawand in saying this I earnestly deprecate the\n",
    "polished indignation of your Fifth Avenue dandieswere the Western\n",
    "miners.  Their wide-brimmed hats, which shaded their faces from the sun\n",
    "and protected them from the rain, and the cloak, which is by far the most\n",
    "beautiful piece of drapery ever invented, may well be dwelt on with\n",
    "admiration.  Their high boots, too, were sensible and practical.  They\n",
    "wore only what was comfortable, and therefore beautiful.  As I looked at\n",
    "them I could not help thinking with regret of the time when these\n",
    "picturesque miners would have made their fortunes and would go East to\n",
    "assume again all the abominations of modern fashionable attire.  Indeed,\n",
    "so concerned was I that I made some of them promise that when they again\n",
    "appeared in the more crowded scenes of Eastern civilisation they would\n",
    "still continue to wear their lovely costume.  But I do not believe they\n",
    "will.\n",
    "\n",
    "Now, what America wants to-day is a school of rational art.  Bad art is a\n",
    "great deal worse than no art at all.  You must show your workmen\n",
    "specimens of good work so that they come to know what is simple and true\n",
    "and beautiful.  To that end I would have you have a museum attached to\n",
    "these schoolsnot one of those dreadful modern institutions where there\n",
    "is a stuffed and very dusty giraffe, and a case or two of fossils, but a\n",
    "place where there are gathered examples of art decoration from various\n",
    "periods and countries.  Such a place is the South Kensington Museum in\n",
    "London, whereon we build greater hopes for the future than on any other\n",
    "one thing.  There I go every Saturday night, when the museum is open\n",
    "later than usual, to see the handicraftsman, the wood-worker, the\n",
    "glass-blower and the worker in metals.  And it is here that the man of\n",
    "refinement and culture comes face to face with the workman who ministers\n",
    "to his joy.  He comes to know more of the nobility of the workman, and\n",
    "the workman, feeling the appreciation, comes to know more of the nobility\n",
    "of his work.\n",
    "\n",
    "You have too many white walls.  More colour is wanted.  You should have\n",
    "such men as Whistler among you to teach you the beauty and joy of colour.\n",
    "Take Mr. Whistlers Symphony in White, which you no doubt have imagined\n",
    "to be something quite bizarre.  It is nothing of the sort.  Think of a\n",
    "cool grey sky flecked here and there with white clouds, a grey ocean and\n",
    "three wonderfully beautiful figures robed in white, leaning over the\n",
    "water and dropping white flowers from their fingers.  Here is no\n",
    "extensive intellectual scheme to trouble you, and no metaphysics of which\n",
    "we have had quite enough in art.  But if the simple and unaided colour\n",
    "strike the right key-note, the whole conception is made clear.  I regard\n",
    "Mr. Whistlers famous Peacock Room as the finest thing in colour and art\n",
    "decoration which the world has known since Correggio painted that\n",
    "wonderful room in Italy where the little children are dancing on the\n",
    "walls.  Mr. Whistler finished another room just before I came awaya\n",
    "breakfast room in blue and yellow.  The ceiling was a light blue, the\n",
    "cabinet-work and the furniture were of a yellow wood, the curtains at the\n",
    "windows were white and worked in yellow, and when the table was set for\n",
    "breakfast with dainty blue china nothing can be conceived at once so\n",
    "simple and so joyous.\n",
    "\n",
    "The fault which I have observed in most of your rooms is that there is\n",
    "apparent no definite scheme of colour.  Everything is not attuned to a\n",
    "key-note as it should be.  The apartments are crowded with pretty things\n",
    "which have no relation to one another.  Again, your artists must decorate\n",
    "what is more simply useful.  In your art schools I found no attempt to\n",
    "decorate such things as the vessels for water.  I know of nothing uglier\n",
    "than the ordinary jug or pitcher.  A museum could be filled with the\n",
    "different kinds of water vessels which are used in hot countries.  Yet we\n",
    "continue to submit to the depressing jug with the handle all on one side.\n",
    "I do not see the wisdom of decorating dinner-plates with sunsets and\n",
    "soup-plates with moonlight scenes.  I do not think it adds anything to\n",
    "the pleasure of the canvas-back duck to take it out of such glories.\n",
    "Besides, we do not want a soup-plate whose bottom seems to vanish in the\n",
    "distance.  One feels neither safe nor comfortable under such conditions.\n",
    "In fact, I did not find in the art schools of the country that the\n",
    "difference was explained between decorative and imaginative art.\n",
    "\n",
    "The conditions of art should be simple.  A great deal more depends upon\n",
    "the heart than upon the head.  Appreciation of art is not secured by any\n",
    "elaborate scheme of learning.  Art requires a good healthy atmosphere.\n",
    "The motives for art are still around about us as they were round about\n",
    "the ancients.  And the subjects are also easily found by the earnest\n",
    "sculptor and the painter.  Nothing is more picturesque and graceful than\n",
    "a man at work.  The artist who goes to the childrens playground, watches\n",
    "them at their sport and sees the boy stoop to tie his shoe, will find the\n",
    "same themes that engaged the attention of the ancient Greeks, and such\n",
    "observation and the illustrations which follow will do much to correct\n",
    "that foolish impression that mental and physical beauty are always\n",
    "divorced.\n",
    "\n",
    "To you, more than perhaps to any other country, has Nature been generous\n",
    "in furnishing material for art workers to work in.  You have marble\n",
    "quarries where the stone is more beautiful in colour than any the Greeks\n",
    "ever had for their beautiful work, and yet day after day I am confronted\n",
    "with the great building of some stupid man who has used the beautiful\n",
    "material as if it were not precious almost beyond speech.  Marble should\n",
    "not be used save by noble workmen.  There is nothing which gave me a\n",
    "greater sense of barrenness in travelling through the country than the\n",
    "entire absence of wood carving on your houses.  Wood carving is the\n",
    "simplest of the decorative arts.  In Switzerland the little barefooted\n",
    "boy beautifies the porch of his fathers house with examples of skill in\n",
    "this direction.  Why should not American boys do a great deal more and\n",
    "better than Swiss boys?\n",
    "\n",
    "There is nothing to my mind more coarse in conception and more vulgar in\n",
    "execution than modern jewellery.  This is something that can easily be\n",
    "corrected.  Something better should be made out of the beautiful gold\n",
    "which is stored up in your mountain hollows and strewn along your river\n",
    "beds.  When I was at Leadville and reflected that all the shining silver\n",
    "that I saw coming from the mines would be made into ugly dollars, it made\n",
    "me sad.  It should be made into something more permanent.  The golden\n",
    "gates at Florence are as beautiful to-day as when Michael Angelo saw\n",
    "them.\n",
    "\n",
    "We should see more of the workman than we do.  We should not be content\n",
    "to have the salesman stand between usthe salesman who knows nothing of\n",
    "what he is selling save that he is charging a great deal too much for it.\n",
    "And watching the workman will teach that most important lessonthe\n",
    "nobility of all rational workmanship.\n",
    "\n",
    "I said in my last lecture that art would create a new brotherhood among\n",
    "men by furnishing a universal language.  I said that under its beneficent\n",
    "influences war might pass away.  Thinking this, what place can I ascribe\n",
    "to art in our education?  If children grow up among all fair and lovely\n",
    "things, they will grow to love beauty and detest ugliness before they\n",
    "know the reason why.  If you go into a house where everything is coarse,\n",
    "you find things chipped and broken and unsightly.  Nobody exercises any\n",
    "care.  If everything is dainty and delicate, gentleness and refinement of\n",
    "manner are unconsciously acquired.  When I was in San Francisco I used to\n",
    "visit the Chinese Quarter frequently.  There I used to watch a great\n",
    "hulking Chinese workman at his task of digging, and used to see him every\n",
    "day drink his tea from a little cup as delicate in texture as the petal\n",
    "of a flower, whereas in all the grand hotels of the land, where thousands\n",
    "of dollars have been lavished on great gilt mirrors and gaudy columns, I\n",
    "have been given my coffee or my chocolate in cups an inch and a quarter\n",
    "thick.  I think I have deserved something nicer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1.\n",
    "Run the code as is to see a made-up Oscar Wilde passage using a training text written by him. The tool currently uses the n-gram statistical model with a word sequence length of 3 (e.g., “I made the”) to make predictions.\n",
    "\n",
    "\n",
    "2.\n",
    "Change sequence_length to 1 so that we use the bag-of-words model. For the purposes of this function, we’ll pick each next word randomly from the 20 most common words.\n",
    "\n",
    "Run the code again to see how bag of words compares with the n-gram model on language prediction. Not so great, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "must more when do or on or there by your the with such have but by made that beautiful art he such or are be must have great with beautiful more they the than it of that but with in they his is he think we their there your\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque, Counter\n",
    "#from document import oscar_wilde_thoughts\n",
    "\n",
    "# Change sequence_length:\n",
    "#sequence_length = 3\n",
    "\n",
    "sequence_length = 1\n",
    "\n",
    "class MarkovChain:\n",
    "  def __init__(self):\n",
    "    self.lookup_dict = defaultdict(list)\n",
    "    self.most_common = []\n",
    "    self._seeded = False\n",
    "    self.__seed_me()\n",
    "\n",
    "  def __seed_me(self, rand_seed=None):\n",
    "    if self._seeded is not True:\n",
    "      try:\n",
    "        if rand_seed is not None:\n",
    "          random.seed(rand_seed)\n",
    "        else:\n",
    "          random.seed()\n",
    "        self._seeded = True\n",
    "      except NotImplementedError:\n",
    "        self._seeded = False\n",
    "    \n",
    "  def add_document(self, str):\n",
    "    preprocessed_list = self._preprocess(str)\n",
    "    self.most_common = Counter(preprocessed_list).most_common(50)\n",
    "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "    for pair in pairs:\n",
    "      self.lookup_dict[pair[0]].append(pair[1])\n",
    "  \n",
    "  def _preprocess(self, str):\n",
    "    cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    return tokenized\n",
    "\n",
    "  def __generate_tuple_keys(self, data):\n",
    "    if len(data) < sequence_length:\n",
    "      return\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "      yield [ data[i], data[i + 1] ]\n",
    "      \n",
    "  def generate_text(self, max_length=50):\n",
    "    context = deque()\n",
    "    output = []\n",
    "    if len(self.lookup_dict) > 0:\n",
    "      self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "      chain_head = [list(self.lookup_dict)[0]]\n",
    "      context.extend(chain_head)\n",
    "      if sequence_length > 1:\n",
    "        while len(output) < (max_length - 1):\n",
    "          next_choices = self.lookup_dict[context[-1]]\n",
    "          if len(next_choices) > 0:\n",
    "            next_word = random.choice(next_choices)\n",
    "            context.append(next_word)\n",
    "            output.append(context.popleft())\n",
    "          else:\n",
    "            break\n",
    "        output.extend(list(context))\n",
    "      else:\n",
    "        while len(output) < (max_length - 1):\n",
    "          next_choices = [word[0] for word in self.most_common]\n",
    "          next_word = random.choice(next_choices)\n",
    "          output.append(next_word)\n",
    "    return \" \".join(output)\n",
    "\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(oscar_wilde_thoughts)\n",
    "random_oscar_wilde = my_markov.generate_text()\n",
    "print(random_oscar_wilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Bag-of-Words\n",
    "You made it! And you’ve learned plenty about the bag-of-words language model along the way:\n",
    "\n",
    "* Bag-of-words (BoW) — also referred to as the unigram model — is a statistical language model based on word count.\n",
    "\n",
    "\n",
    "* There are loads of real-world applications for BoW.\n",
    "\n",
    "\n",
    "* BoW can be implemented as a Python dictionary with each key set to a word and each value set to the number of times that word appears in a text.\n",
    "\n",
    "\n",
    "* For BoW, training data is the text that is used to build a BoW model.\n",
    "\n",
    "\n",
    "* BoW test data is the new text that is converted to a BoW vector using a trained features dictionary.\n",
    "\n",
    "\n",
    "* A feature vector is a numeric depiction of an item’s salient features.\n",
    "\n",
    "\n",
    "* Feature extraction (or vectorization) is the process of turning text into a BoW vector.\n",
    "\n",
    "\n",
    "* A features dictionary is a mapping of each unique word in the training data to a unique index. This is used to build out BoW vectors.\n",
    "\n",
    "\n",
    "* BoW has less data sparsity than other statistical models. It also suffers less from overfitting.\n",
    "\n",
    "\n",
    "* BoW has higher perplexity than other models, making it less ideal for language prediction.\n",
    "\n",
    "\n",
    "* One solution to overfitting is language smoothing, in which a bit of probability is taken from known words and allotted to unknown words.\n",
    "\n",
    "\n",
    "* The spam data for this lesson were taken from the UCI Machine Learning Repository.\n",
    "\n",
    "\n",
    "\n",
    "Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spam_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2a5966039c7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspam_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_spam_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_doc_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m test_text = \"\"\"\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spam_data'"
     ]
    }
   ],
   "source": [
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels, training_docs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "test_text = \"\"\"\n",
    "Play around with the spam classifier!\n",
    "\"\"\"\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "training_vectors = bow_vectorizer.fit_transform(training_docs)\n",
    "test_vectors = bow_vectorizer.transform([test_text])\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.predict(test_vectors)\n",
    "\n",
    "print(\"Looks like a normal email!\" if predictions[0] == 0 else \"You've got spam!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
